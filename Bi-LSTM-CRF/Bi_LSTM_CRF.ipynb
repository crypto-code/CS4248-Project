{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wRGihu_eeDu",
        "outputId": "74f2d0fc-8360-42ec-c782-2392a57aa5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, Reshape, Bidirectional, concatenate, Flatten, Concatenate, Add, Average, Lambda\n",
        "from keras_contrib.layers import CRF\n",
        "from future.utils import iteritems\n",
        "import keras as k\n",
        "import os, sys, pickle, glob\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCzj4LFWeenZ"
      },
      "outputs": [],
      "source": [
        "class LSTM_CRF():\n",
        "    \n",
        "    def __init__(self, max_words, vocab_size, tag_size, output_dim):     \n",
        "        self.max_words = max_words\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_size = tag_size\n",
        "        self.model = None\n",
        "        self.output_dim = output_dim\n",
        "        \n",
        "    def define_model(self, word_embedding_dim=50, lstm_cell=20):\n",
        "        word_input = Input(shape=(self.max_words,), name=\"word_input\")\n",
        "        word = Embedding(input_dim=self.vocab_size, output_dim=word_embedding_dim)(word_input)\n",
        "        \n",
        "        pos_tag_input = Input(shape=(self.max_words,), name=\"pos_tag_input\")\n",
        "        pos_tag = Embedding(input_dim=self.tag_size, output_dim=word_embedding_dim)(pos_tag_input)\n",
        "        pos_tag = Lambda(lambda x: x * 5)(pos_tag)\n",
        "        concat = Average()([word, pos_tag])\n",
        "\n",
        "        model = Bidirectional(\n",
        "            LSTM(lstm_cell, return_sequences=True),\n",
        "            merge_mode='concat'\n",
        "        )(concat)\n",
        "        model = TimeDistributed(\n",
        "            Dense(self.output_dim, activation='softmax')\n",
        "        )(model)\n",
        "        crf = CRF(self.output_dim, name=\"output\")\n",
        "        output = crf(model)\n",
        "\n",
        "        m = Model(inputs=[word_input,pos_tag_input], outputs=output)\n",
        "        adam = k.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999)\n",
        "\n",
        "        m.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])  \n",
        "\n",
        "        self.model = m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eWvjTc5fWO3"
      },
      "outputs": [],
      "source": [
        "with open('data/train_augmented_bio_annotations.npy', 'rb') as f:\n",
        "    annotation = np.load(f, allow_pickle=True)\n",
        "with open('data/train_augmented_bio_sentences.npy', 'rb') as f:\n",
        "    text = np.load(f,allow_pickle=True)\n",
        "with open('data/train_pos_tag.npy', 'rb') as f:\n",
        "    pos_tags = np.load(f,allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iH7O_Pkfxcu"
      },
      "outputs": [],
      "source": [
        "word_to_ix = {}\n",
        "for sentence in text:\n",
        "    for word in sentence:\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dWmBdSA2d80"
      },
      "outputs": [],
      "source": [
        "pos_tag_to_ix = {}\n",
        "for pos in pos_tags:\n",
        "  for tag in pos:\n",
        "    if tag not in pos_tag_to_ix:\n",
        "      pos_tag_to_ix[tag] = len(pos_tag_to_ix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HV5ISJ6VyQpo",
        "outputId": "cf6540a6-a24e-4a44-a90a-42ecd200ac81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'B_Task',\n",
              " 1: 'I_Task',\n",
              " 2: 'B_Process',\n",
              " 3: 'I_Process',\n",
              " 4: 'B_Material',\n",
              " 5: 'I_Material',\n",
              " 6: 'O'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "tag_to_ix = {\"B_Task\": 0, \"I_Task\": 1, \"B_Process\": 2, \"I_Process\": 3, \"B_Material\": 4, \"I_Material\": 5, \"O\": 6}\n",
        "idx_to_tag = {v: k for k, v in iteritems(tag_to_ix)}\n",
        "idx_to_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wat9RnDeumOJ"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "maxlen = max([len(s) for s in text])\n",
        "n_words = len(word_to_ix)\n",
        "n_tags = len(tag_to_ix) \n",
        "\n",
        "X_word = [[word_to_ix[w] for w in s] for s in text]\n",
        "X_word = pad_sequences(maxlen=maxlen, sequences=X_word, padding=\"post\",value=n_words)\n",
        "\n",
        "X_tag = [[pos_tag_to_ix[w] for w in s] for s in pos_tags]\n",
        "X_tag = pad_sequences(maxlen=maxlen, sequences=X_tag, padding=\"post\",value=len(pos_tag_to_ix))\n",
        "\n",
        "y = [[tag_to_ix[w] for w in s] for s in annotation]\n",
        "y = pad_sequences(maxlen=maxlen, sequences=y, padding=\"post\", value=tag_to_ix[\"O\"])\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in y]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHyzRmKufOdp",
        "outputId": "a67203e3-2447-4f48-d31a-62fb32be5528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
            "  warnings.warn('CRF.loss_function is deprecated '\n",
            "/usr/local/lib/python3.7/dist-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
            "  warnings.warn('CRF.accuracy is deprecated and it '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_ops.py:2509: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "pos_tag_input (InputLayer)      (None, 297)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_input (InputLayer)         (None, 297)          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, 297, 50)      2150        pos_tag_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, 297, 50)      412050      word_input[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 297, 50)      0           embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "average_1 (Average)             (None, 297, 50)      0           embedding_1[0][0]                \n",
            "                                                                 lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 297, 40)      11360       average_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "time_distributed_1 (TimeDistrib (None, 297, 7)       287         bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "output (CRF)                    (None, 297, 7)       119         time_distributed_1[0][0]         \n",
            "==================================================================================================\n",
            "Total params: 425,966\n",
            "Trainable params: 425,966\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "lstm = LSTM_CRF(max_words=maxlen, vocab_size=n_words+1, tag_size=len(pos_tag_to_ix)+1, output_dim=n_tags)\n",
        "lstm.define_model()\n",
        "lstm.model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myDSOzWkv4C0",
        "outputId": "94a6b20c-35bf-41df-8272-d31e4be8d1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "Epoch 1/7\n",
            "8412/8412 [==============================] - 124s 15ms/step - loss: 0.3173 - crf_viterbi_accuracy: 0.9018 - accuracy: 0.0038\n",
            "Epoch 2/7\n",
            "8412/8412 [==============================] - 121s 14ms/step - loss: 0.1854 - crf_viterbi_accuracy: 0.9198 - accuracy: 0.0038\n",
            "Epoch 3/7\n",
            "8412/8412 [==============================] - 136s 16ms/step - loss: 0.1776 - crf_viterbi_accuracy: 0.9198 - accuracy: 0.0038\n",
            "Epoch 4/7\n",
            "8412/8412 [==============================] - 124s 15ms/step - loss: 0.1249 - crf_viterbi_accuracy: 0.9199 - accuracy: 0.0038\n",
            "Epoch 5/7\n",
            "8412/8412 [==============================] - 121s 14ms/step - loss: 0.0464 - crf_viterbi_accuracy: 0.9474 - accuracy: 0.0038\n",
            "Epoch 6/7\n",
            "8412/8412 [==============================] - 121s 14ms/step - loss: 0.0156 - crf_viterbi_accuracy: 0.9713 - accuracy: 0.0038\n",
            "Epoch 7/7\n",
            "8412/8412 [==============================] - 120s 14ms/step - loss: -0.0052 - crf_viterbi_accuracy: 0.9823 - accuracy: 0.0038\n"
          ]
        }
      ],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "history = lstm.model.fit([X_word,X_tag] , np.array(y), batch_size=16, epochs=7, verbose=1)\n",
        "\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_7XH-b9xdYh"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "    accuracy = history.history['accuracy']\n",
        "    # val_accuracy = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    # val_loss = history.history['val_loss']\n",
        "    x = range(1, len(accuracy) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, accuracy, 'b', label='Training acc')\n",
        "    # plt.plot(x, val_accuracy, 'r', label='Validation acc')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    # plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afw5prCtAFjG"
      },
      "source": [
        "## Testing ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StxcNlfU7xhc"
      },
      "outputs": [],
      "source": [
        "with open('data/test_bio_annotations.npy', 'rb') as f:\n",
        "    dev_annotation = np.load(f, allow_pickle=True)\n",
        "with open('data/test_bio_sentences.npy', 'rb') as f:\n",
        "    dev_text = np.load(f,allow_pickle=True)\n",
        "with open('data/test_pos_tag.npy', 'rb') as f:\n",
        "    dev_pos_tag = np.load(f,allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ztgD_yi7xhc"
      },
      "outputs": [],
      "source": [
        "X_dev = [[word_to_ix[w] if w in word_to_ix else n_words for w in s] for s in dev_text]\n",
        "X_dev = pad_sequences(maxlen=maxlen, sequences=X_dev, padding=\"post\",value=n_words)\n",
        "\n",
        "X_dev_tag = [[pos_tag_to_ix[w] if w in pos_tag_to_ix else len(pos_tag_to_ix) for w in s] for s in dev_pos_tag]\n",
        "X_dev_tag = pad_sequences(maxlen=maxlen, sequences=X_dev_tag, padding=\"post\",value=len(pos_tag_to_ix))\n",
        "\n",
        "y_dev = [[tag_to_ix[w] for w in s] for s in dev_annotation]\n",
        "y_dev = pad_sequences(maxlen=maxlen, sequences=y_dev, padding=\"post\", value=tag_to_ix[\"O\"])\n",
        "y_dev = [to_categorical(i, num_classes=n_tags) for i in y_dev]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_fmcM9Z7xhc",
        "outputId": "0f59cd3e-6650-4914-8214-bc148fa4aaae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 297, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "y_pred = lstm.model.predict([X_dev,X_dev_tag])\n",
        "y_pred.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W6-vALO7xhd"
      },
      "outputs": [],
      "source": [
        "def pred2label(pred, text):\n",
        "    out = []\n",
        "    for i,pred_i in enumerate(pred):\n",
        "        for j,p in enumerate(pred_i):\n",
        "            p_i = np.argmax(p)\n",
        "            if('Task' in idx_to_tag[p_i]):\n",
        "              out.append('Task')\n",
        "            elif 'Material' in idx_to_tag[p_i]:\n",
        "              out.append('Material')\n",
        "            elif 'Process' in idx_to_tag[p_i]:\n",
        "              out.append('Process')\n",
        "            else:\n",
        "              out.append(idx_to_tag[p_i])\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxLrjvXC7xhd"
      },
      "outputs": [],
      "source": [
        "pred_labels = pred2label(y_pred, dev_text)\n",
        "test_labels = pred2label(y_dev, dev_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gHJ-B2G7xhe"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, f1_score\n",
        "report = classification_report(y_pred=pred_labels, y_true=test_labels)\n",
        "print(report)\n",
        "print(f1_score(y_pred=pred_labels, y_true=test_labels,average = \"macro\"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Bi-LSTM-CRF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}